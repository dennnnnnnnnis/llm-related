{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "* Extract audio from the videos.\n",
    "* Transcribe the audio to text.\n",
    "* Create and save embeddings generated from transcriptions (Vector DB).\n",
    "* Retrieve similar text from the Vector DB.\n",
    "* Utilize the similar content as context for Large Language Model (LLM) response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from yt_dlp import YoutubeDL\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from langchain.document_loaders import JSONLoader, DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/huggingface-models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the videos (audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_LINK = 'https://www.youtube.com/playlist?list=PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6'\n",
    "AUDIO_INPUTS = '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/audio_inputs'\n",
    "TRANSCRIPTIONS = '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/transcriptions'\n",
    "\n",
    "cache_dir = '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/huggingface-models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] Extracting URL: https://www.youtube.com/playlist?list=PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6\n",
      "[youtube:tab] PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6: Downloading webpage\n",
      "[youtube:tab] PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6: Redownloading playlist API JSON with unavailable videos\n",
      "[download] Downloading playlist: Sequence Models (Course 5 of the Deep Learning Specialization)\n",
      "[youtube:tab] PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6 page 1: Downloading API JSON\n",
      "[youtube:tab] Playlist Sequence Models (Course 5 of the Deep Learning Specialization): Downloading 6 items of 6\n",
      "[download] Downloading item 1 of 6\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=_i3aqgKVNQI\n",
      "[youtube] _i3aqgKVNQI: Downloading webpage\n",
      "[youtube] _i3aqgKVNQI: Downloading tv client config\n",
      "[youtube] _i3aqgKVNQI: Downloading player 612f74a3-main\n",
      "[youtube] _i3aqgKVNQI: Downloading tv player API JSON\n",
      "[youtube] _i3aqgKVNQI: Downloading ios player API JSON\n",
      "[youtube] _i3aqgKVNQI: Downloading m3u8 information\n",
      "[info] _i3aqgKVNQI: Downloading 1 format(s): 140\n",
      "[download] Destination: /Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/audio_inputs/C5W3L01 Basic Models [_i3aqgKVNQI].m4a\n",
      "[download] 100% of    5.73MiB in 00:00:02 at 2.28MiB/s   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: _i3aqgKVNQI: writing DASH m4a. Only some players support this container. Install ffmpeg to fix this automatically\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] Downloading item 2 of 6\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=Er2ucMxjdHE\n",
      "[youtube] Er2ucMxjdHE: Downloading webpage\n",
      "[youtube] Er2ucMxjdHE: Downloading tv client config\n",
      "[youtube] Er2ucMxjdHE: Downloading tv player API JSON\n",
      "[youtube] Er2ucMxjdHE: Downloading ios player API JSON\n",
      "[youtube] Er2ucMxjdHE: Downloading m3u8 information\n",
      "[info] Er2ucMxjdHE: Downloading 1 format(s): 140\n",
      "[download] Destination: /Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/audio_inputs/C5W3L02 Picking the most likely sentence [Er2ucMxjdHE].m4a\n",
      "[download] 100% of    8.12MiB in 00:00:03 at 2.52MiB/s   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Er2ucMxjdHE: writing DASH m4a. Only some players support this container. Install ffmpeg to fix this automatically\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] Downloading item 3 of 6\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=DejHQYAGb7Q\n",
      "[youtube] DejHQYAGb7Q: Downloading webpage\n",
      "[youtube] DejHQYAGb7Q: Downloading tv client config\n",
      "[youtube] DejHQYAGb7Q: Downloading tv player API JSON\n",
      "[youtube] DejHQYAGb7Q: Downloading ios player API JSON\n",
      "[youtube] DejHQYAGb7Q: Downloading m3u8 information\n",
      "[info] DejHQYAGb7Q: Downloading 1 format(s): 140\n",
      "[download] Destination: /Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/audio_inputs/C5W3L06 Bleu Score (Optional) [DejHQYAGb7Q].m4a\n",
      "[download] 100% of   15.22MiB in 00:00:06 at 2.52MiB/s     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: DejHQYAGb7Q: writing DASH m4a. Only some players support this container. Install ffmpeg to fix this automatically\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] Downloading item 4 of 6\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=SysgYptB198\n",
      "[youtube] SysgYptB198: Downloading webpage\n",
      "[youtube] SysgYptB198: Downloading tv client config\n",
      "[youtube] SysgYptB198: Downloading tv player API JSON\n",
      "[youtube] SysgYptB198: Downloading ios player API JSON\n",
      "[youtube] SysgYptB198: Downloading m3u8 information\n",
      "[info] SysgYptB198: Downloading 1 format(s): 140\n",
      "[download] Destination: /Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/audio_inputs/C5W3L07 Attention Model Intuition [SysgYptB198].m4a\n",
      "[download] 100% of    8.81MiB in 00:00:02 at 3.08MiB/s   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: SysgYptB198: writing DASH m4a. Only some players support this container. Install ffmpeg to fix this automatically\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] Downloading item 5 of 6\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=quoGRI-1l0A\n",
      "[youtube] quoGRI-1l0A: Downloading webpage\n",
      "[youtube] quoGRI-1l0A: Downloading tv client config\n",
      "[youtube] quoGRI-1l0A: Downloading tv player API JSON\n",
      "[youtube] quoGRI-1l0A: Downloading ios player API JSON\n",
      "[youtube] quoGRI-1l0A: Downloading m3u8 information\n",
      "[info] quoGRI-1l0A: Downloading 1 format(s): 140\n",
      "[download] Destination: /Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/audio_inputs/C5W3L08 Attention Model [quoGRI-1l0A].m4a\n",
      "[download] 100% of   11.25MiB in 00:00:04 at 2.52MiB/s   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: quoGRI-1l0A: writing DASH m4a. Only some players support this container. Install ffmpeg to fix this automatically\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] Downloading item 6 of 6\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=vm2SI8AJY0s\n",
      "[youtube] vm2SI8AJY0s: Downloading webpage\n",
      "[youtube] vm2SI8AJY0s: Downloading tv client config\n",
      "[youtube] vm2SI8AJY0s: Downloading tv player API JSON\n",
      "[youtube] vm2SI8AJY0s: Downloading ios player API JSON\n",
      "[youtube] vm2SI8AJY0s: Downloading m3u8 information\n",
      "[info] vm2SI8AJY0s: Downloading 1 format(s): 140\n",
      "[download] Destination: /Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/audio_inputs/C5W3L09 SpeechRecog [vm2SI8AJY0s].m4a\n",
      "[download] 100% of    8.24MiB in 00:00:03 at 2.37MiB/s   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: vm2SI8AJY0s: writing DASH m4a. Only some players support this container. Install ffmpeg to fix this automatically\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] Finished downloading playlist: Sequence Models (Course 5 of the Deep Learning Specialization)\n"
     ]
    }
   ],
   "source": [
    "path = pathlib.Path(AUDIO_INPUTS)\n",
    "# format 140: audio-only(m4a), paths: ... sets the download directory\n",
    "with YoutubeDL(params={'format': '140', \"paths\": {\"home\": path.as_posix()}}) as ydl:\n",
    "    ydl.download(VIDEO_LINK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/audio_inputs/C5W3L01 Basic Models [_i3aqgKVNQI].m4a',\n",
       " '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/audio_inputs/C5W3L02 Picking the most likely sentence [Er2ucMxjdHE].m4a',\n",
       " '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/audio_inputs/C5W3L06 Bleu Score (Optional) [DejHQYAGb7Q].m4a',\n",
       " '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/audio_inputs/C5W3L07 Attention Model Intuition [SysgYptB198].m4a',\n",
       " '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/audio_inputs/C5W3L08 Attention Model [quoGRI-1l0A].m4a',\n",
       " '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/audio_inputs/C5W3L09 SpeechRecog [vm2SI8AJY0s].m4a']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_paths = sorted(glob.glob(AUDIO_INPUTS + '/*.m4a'))\n",
    "audio_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribe the audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9e69c5b5284052bc033b7975b6d09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/339 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# default configuration based on distil-whisper/distil-small.en model card on huggingface\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"distil-whisper/distil-small.en\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype, \n",
    "    low_cpu_mem_usage=True, \n",
    "    use_safetensors=True,\n",
    "    cache_dir = cache_dir\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, cache_dir = cache_dir)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd4eaee66054d2fbc3543a9287a63c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/.venv/lib/python3.13/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/.venv/lib/python3.13/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/.venv/lib/python3.13/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/.venv/lib/python3.13/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/.venv/lib/python3.13/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/.venv/lib/python3.13/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# save the transcriptions in .txt file\n",
    "for audio_path in tqdm(audio_paths):\n",
    "\n",
    "    json_dict = {}\n",
    "\n",
    "    transcriptions = pipe(audio_path, chunk_length_s=15, batch_size=32, return_timestamps=True)\n",
    "\n",
    "    text = transcriptions['text']\n",
    "    video_title = audio_path.split('/')[-1][:-4]\n",
    "\n",
    "    file_path = TRANSCRIPTIONS + f'/{video_title}.txt'\n",
    "\n",
    "    with open(file_path, \"wb\") as outfile:\n",
    "        outfile.write(text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 671.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\n",
    "    path = TRANSCRIPTIONS + '/',\n",
    "    glob=\"./*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print('Documents: ',len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/transcriptions/C5W3L06 Bleu Score (Optional) [DejHQYAGb7Q].txt'}, page_content=\" One of the challenges of machine translation is that given a French sentence there could be multiple English translations that are equally good translations in that French sentence. So how do you evaluate a machine translation system if there are multiple equally good answers? Unlike say image recognition where there's one right answer, if you just measure accuracy, If there are multiple great answers how do you measure accuracy? The way this is done conventionally is with something called the blue score. So in this optional video I want to share of you, I want to give you a sense of how the blue score works. Let's say you are given a French sentence, the shyest of the tabby, and you are given a reference human-generated translation of this, which is the cat is on the mat, but there are multiple pretty good translations of this so different human different person might translate it as there is a cat on the mat and both of these are actually just fine perfectly fine translations of the French sentence. What the blue score does is given a machine generated translation it allows you to automatically compute a score that measures how good is that machine translation. And the intuition is so long as the machine generated translation is pretty close to any of the references provided by humans, then it would get a high blue score. Blue, by the way, stands for bilingual evaluation. evaluation, understudy. So in the theater world, an understudy is someone that learns the role of a more senior actor so they can take over the role of the more senior actor if necessary and motivation for blue is that whereas you could ask human evaluators to evaluate the machine translation system, the blue score is an understudy. It could be a substitute for having humans evaluate every output of a machine translation system. So the blue school was due to Keshoe Papinani, Salim Ruka's Todd Ward and Wei Jing Liu. This paper has been incredibly influential and it's actually quite a reasonable and it's actually quite a readable paper so I encourage you to take a look if you have time. So the intuition behind the blue score is we're going to look at the machine generated output and see if the types of words it generates appear in at least one of the human generated references. And so these human generated references would be provided as part of the depth set or as part of the test set. Now let's look at the somewhat extreme example. Let's say that the machine translation system abbreviating machine translation as MT. So the machine translation of the MT outputs is D-d-d-d-d-d-d-d-d-d-d-d-d-d-d-d-d-d-d-d. So this is a clearly a pretty terrible translation. So one way to measure how good the machine translation output is is to look at each of the words in the output and see if it appears in the references. And so this could be called a precision of the machine translation output. And in this case there are seven words in the machine translation output and every one of these seven words appears in either reference one or reference two. Right. So the word the appears in both references. So each of these words looks like a pretty good word to include. So this will have a precision of 7 over 7. It looks like it is a great position. So this is why the basic position measure of what fraction of the words in the empty output also appear in the references this is not a particularly useful measure because it seems to imply that this empty output has very high position. So instead what we're going to use is a modified precision measure in which we will give each word credit only up to the maximum number of times it appears in the reference sentences. So in reference 1, the word D appears twice. In reference 2, the word D appears just once. So two is bigger than one. And so we're going to say that the word D gets credit up to twice. So with a modified position, we will say that it gets a score of two out of seven. Because out of seven works would give it a two credits for appearing. So here the denominator is the count of the number of times the word D appears The seven words in total and the numerator is the count of the number of times the word D appears but we clip discount and we take a max or a clip discount at two. So this gives us the modified precision measure. Now so far we've been looking at words in isolation. In the blue school you don't want to just look at isolated words. You maybe want to just look at isolated words you maybe want to look at pairs of words as well. Let's define a portion of the blue school on biograms and biograms just means pairs of words appearing next to each other. So now let's see how we could use biograms to define the blue score. And this would just be a portion of the final blue school. And we'll take unigrams or single words as well as biograms, which means pairs of words into count, as well as maybe even longer sequences of words, such as trigrams, which means three words appearing together. So let's continue our example from before. We have the same reference one in reference two. But now let's say the machine translation of the empty system has a slightly better output. The cat on the mat. Still not a great translation, but maybe better than the last one. So here, the possible biograms are, well, there's the cat, when they ignore case, and then there's cat D, that's another biogram, and then there's D cat again, but I've already had that, so let's skip that, and then cat on, as next one and then on the and the map. So these are the biograms in the machine translation output. And so let's count up how many times each of these diagrams appear. The cat appears twice cat D appears once, and the others all appear just once. And then finally, let's define the clipped count. So count and then substrip clip. And to define that, let's take this column with numbers but give our algorithm credit only up to the maximum number of times that that biogram appears in either reference one or reference two. So the cat appears a maximum of once in either of the references. So I'm going to clip that count to one. Cat V, well it doesn't appear in reference one or reference to, so clip that to zero. Cat on, yep, that appears once, we give it credit for once, on D appears once, give that credit for once, and D-mat appears one. So these are the clipped counts. We're taking all the counts and clipping them, really reducing them to be no more than the number of times that biogram appears in at least one of the references. And then finally our modified biogram position will be the sum of the count clips, so that's one, two, three, four, divided by the total number of bygrams, that's two, three, four, five, six. So four of the six or two-thirds is the modified precision on biograms. So let's just formalize this a little bit further. With what we had developed with on unigrams, we defined this modified precision computed on unigrams as p-substrate one. So p-sensor position and the substrate one here means we're referring to unigrams. But that is defined as sum over the unigrams. So that just means sum over the words that appear in the machine translation output. So this is called Y hat of count clip of that unigram divided by some overall unigrams in the machine translation output of counts number of counts of that unigram. And so this was the, what we had gotten as, I guess, as two out of seven, two slides back. So the one here refers to unigram meaning we're looking at single words in isolation. You can also define PN as the n-gram version instead of a unigram for n-gram so this would be sum over the n-gram so this would be sum over the n-grams in the machine translation output of count clip of that n-grams divided by some over n-grams of the count of that endram. And so these positions, or these modified precision schools, measured on unigrams or on biograms, which you did on a previous slide or on trigrams which are triples of words or even higher values of n for other n grams this allows you to measure the degree to which the machine translation output is similar or maybe overlaps with the references. And one thing that you could pretty convince yourself of is if the empty output is exactly the same as either reference one or reference two, then all of these values P1 and P2 and so on, they'll all be equal to 1.0. So to get a position or a modified position of 1.0, you just have to be exactly equal to one of the references. And sometimes it's possible to achieve this, even if you aren't exactly the same as any of the references, but kind of combined them in a way that hopefully still results in a good translation. Finally... Finally, let's put this together to form the final blue score. So P, Subscript N, is the blue school. So p-subscript n is the blue school computed on n-ground only, or also the modified position, computed on n-grams only. And by convention to compute one number, you compute P1, P2, P3, and P4, and combine them together using the following formula. It's going to be the average, so sum from N equals 1 to 4 of PN and divide that by 4. So basically taking the average. By convention the blue scores defined as e to the this and explanations a linear operate explanation is strictly monotonically increasing operation. And then we actually adjust this with one more factor called the BP penalty. So BP. BP, stands for a brevity penalty. The details maybe aren't super important but just give you a sense. It turns out that if you output very short translations it's easier to get high precision because probably most of the words you appear, probably most of the words you output appear in the references. So, but we don't want translations that are very short. So the BP or the brevity penalty is an adjustment factor that penalizes translation systems that output translations are too short. So the formula for the brevity penalties are following is equal to one if your machine translation system actually outputs things that are longer than the human generated reference output and otherwise is some formula like that that overall penalizes shorter translations. So in the details you can find in this paper. So once again, earlier in this set of courses, you saw the importance of having a single row number evaluation metric. Because that allows you to try out two ideas, see which point the sheet is a higher school, and then try to stick with the one that achieved the highest school. So the reason the blue school was revolutionary for machine translation was because this gave a pretty good, by no means perfect, but pretty good single real number evaluation metric and so that accelerated the progress of the entire field of machine translation. I hope this video gave you a sense of how the Blue School works. In practice, a few people would implement a Blue School from scratch. They're open source implementations you can download and just use to evaluate your own system. But today, Blue School is used to evaluate many systems that generate text such as machine translation systems as well as the example I showed briefly earlier of image capturing systems where you would have a system, have a neural network, generate an image caption, and then use the blue score to see how much that overlaps with maybe a reference caption or multiple reference captions that were generated by people. So the blue score is a useful single row number evaluation metric to use whenever you want your album to generate a piece of text and you want to see whether it has similar meaning as a reference piece of text generated by humans. This is not used for speech recognition because of speech recognition. There's usually one ground truth and you just use other measures to see if you got a speech transcription on pretty much exactly word for word correct but for things like image captioning multiple captions for a picture it could be about equally good or for machine translation there are multiple translations about equally good. The blue score gives you a way to evaluate that automatically and therefore speed up your algebraic development. So with that, I hope you have a sense of how the blue school works.\")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Embedding Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='BAAI/bge-small-en-v1.5', \n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local('/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/rag-optimizations/faiss_videos', embeddings, allow_dangerous_deserialization=True)\n",
    "# db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.save_local('/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/rag-optimizations/faiss_videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aadf5dcc91e4c5e8509c04891ed2581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='b93b2ac5-8f49-4408-8d0a-281bade1a19f', metadata={'source': '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/transcriptions/C5W3L01 Basic Models [_i3aqgKVNQI].txt'}, page_content=\"Although it turns out there are multiple groups coming up with very similar models independently and at about the same time. So two other groups that had done very similar work at about the same time and I think independently of Maudal where all your vinnials exander Tocia of Sammy Benjo and Dimitri Urhan as well as Andrekapathy and Faye Faye. So you've now seen how a basic sequence to sequence model works, how a basic image to sequence or image captioning model works. But there are some differences between how you would run a model like this to generate a sequence compared to how you were synthesizing novel text using a language model. One of the key differences is you don't want to randomly chosen translation. You maybe want the most likely translation. You don't want to randomly chosen caption, maybe not, but you might want the best caption, the most likely caption. So let's see in the next video how you go about generating that.\"),\n",
       " Document(id='e44dbe08-175b-4095-ae37-db157e6583fd', metadata={'source': '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/transcriptions/C5W3L02 Picking the most likely sentence [Er2ucMxjdHE].txt'}, page_content=\"There are some similarities between the sequence-to-sequenced machine translation model and the language models that you have worked with in the first week of this course, but there are some significant differences as well. Let's take a look. So you can think of machine translation as building a conditional language model. Here's what I mean. In language modeling, this was the network we had built in the first week. And this model allows you to estimate the probability of a sentence. That's what a language model does. And you can also use this to generate novel sentences. And sometimes we were writing... generate novel sentences. And sometimes we were writing X1 and X2 here, where in this example, X2 would be equal to Y1 or equal to YAT1 is just a feedback, but X1, X2, and so on weren't important. So just to clean this out for this slide, I'm going to just cross these on. Where X1 could be the vector of all zeros and x2 x3 are just the previous output you were generating. So that was\"),\n",
       " Document(id='00cb4af8-bacf-4df2-b924-32fa0349f5f2', metadata={'source': '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/transcriptions/C5W3L01 Basic Models [_i3aqgKVNQI].txt'}, page_content=\"Hello and welcome to this final week of this course as well as to the final week of this sequence of five courses in the deep learning specialization. You are nearly at the finish line. In this week you hear about sequence to sequence models which are useful for everything from machine translation to speech recognition. The start with the basic models and then later this week you hear about beam search, detention model, and we'll wrap up with discussion of models for audio data like speech. Let's get started. Let's say you want to input a French sentence like Jean Vizique Lafrique in September and you want to translate it to the English sentence Jane is visiting Africa in September. As usual let's use X1 through X in this case 5 to represent the words in the input sequence and we'll use y1 through y6 to represent the words in the output sequence. So how can you train a new network to input the sequence X and output the sequence Y. Well here's something you could do. And the ideas I'm\"),\n",
       " Document(id='16c19ac8-3371-4f72-bf1f-6e9a846d01ca', metadata={'source': '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/transcriptions/C5W3L09 SpeechRecog [vm2SI8AJY0s].txt'}, page_content=\"One of the most exciting developments with sequence to sequence models has been the rise of very accurate speech recognition. We're nearing the end of the course. We want to take just a couple videos to give you a sense of how these sequences sequence models are applied to audio data, such as the speech. So what is the speech recognition problem? You're given an audio clip X and your job is to automatically find a text transcript Y. So an audio clip, if you plot it, looks like this, the horizontal axis here is time, and what a microphone does is it really measures minuscute changes in air pressure, And the way you are hearing my voice right now is that your ear is detecting little changes in air pressure probably generated either by your speakers or by a headset. And so an audio clip like this plots, you know, basically air pressure against time. And if this audio clip is of me saying the quick brown box, then hopefully a speech recognition algorithm can input that audio clip and\"),\n",
       " Document(id='5d408877-1a44-4712-835b-46e79d1d5c95', metadata={'source': '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/transcriptions/C5W3L08 Attention Model [quoGRI-1l0A].txt'}, page_content=\"In the last video, you saw how the attention model allows a neural network to pay attention to only part of an input sentence whilst generating a translation, much like a human translator might. Let's now formalize their intuition into the exact details of how you would implement an attention model. So same as in the previous video let's assume you have an input sentence and you use a bidirectional R&N or bidirectional GRU or bidirectional LSTM to compute features on every word. In practice, GRIUs and LSTMs are often used for this, with maybe LSTMs being more common. And so for the four recurrence you would have a four recurrence first time step, activation backward recurrence, first time step, step, activation, backward occurrence, first time step, activation for the four occurrence, second time step, activation, uh, backward and so on, because I want four of them in. There's a forward fifth time step, a backward fifth time step. Well, you know, we had an A0 here. Technically, we\"),\n",
       " Document(id='110b68c4-2533-4ce4-949a-28d5a0fd3017', metadata={'source': '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/transcriptions/C5W3L09 SpeechRecog [vm2SI8AJY0s].txt'}, page_content=\"due to Alex Gray San Diego Fernandez for Stina Gomez and Jovan Schmidt-Huba. So here's the idea. Let's say the audio clip was of someone saying the quick brown files. We're going to use a, we're going to use a neural network structured like this with an equal number of input axis and output wise. And I've drawn a simple of one uni-directional for the only R&N for this, but in practice this will usually be a bidirectional LSTM or bidirectional GIU and usually a deeper model. But notice that the number of time steps here is very large and in speech recognition, usually the number of input time steps is much bigger than the number of output time steps. So for example, if you have 10 seconds of audio and your features come at a 100 hertz, so 100 samples per second, then a 10-second audio clip would end up with 1,000 inputs. So it's 100 hertz times 10 seconds ends up with 1,000 inputs. But your output might not have a thousand alphabets, might not have a thousand characters. So what do you\"),\n",
       " Document(id='53a95285-8e10-46c1-be34-524210defb8d', metadata={'source': '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/transcriptions/C5W3L02 Picking the most likely sentence [Er2ucMxjdHE].txt'}, page_content=\"a language model. The machine translation model looks as follows. I'm going to use a couple different colors, green and purple, to denote respectively the encoder network in green and the decoder network in purple. And you notice that the decoder network looks pretty much identical to the language model that we had up there. So what the machine translation model is, is very similar to the language model, except that instead of always starting it off with the vector of all zeros, it instead has an encoder network that figures out some representation for the input sentence and it takes that input sentence and starts off the decoded network with representation of the input sentence rather than with the representation of all zeros. So that's why I call this a conditional language model and instead of modeling the probability of any sentence it is now modeling the probability of, say, the output English translation, conditions on some input French sentence. So in other words, you're trying\"),\n",
       " Document(id='f966be99-8985-4ba6-a8ab-eda3f1a0b143', metadata={'source': '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/transcriptions/C5W3L08 Attention Model [quoGRI-1l0A].txt'}, page_content=\"the per exercise, you get to implement and play up the attention model yourself for the date normalization problem so the problem of inputting a date like this this is the date of the Apollo moon landing and normalizing it into standard formats or a date like this and having a new network a sequence a sequence model normalize it to this format. This, by the way, is the birth date of William Shakespeare. Also is believed to be. And what you see in the program exercises, you can train in your network to input dates in you know any of these formats and have it use an attention model to generate a normalized format for these dates. One other thing that's sometimes fun to do is to look at the visualizations of the attention weights. So here's a machine translation example. And here we're plotted in different colors the magnitude of the different attention weights. I don't spend too much time on this, but you find that the corresponding input and output words, you find that the attention\"),\n",
       " Document(id='b1f1de56-59d5-4f31-a615-f70ccb31e3f9', metadata={'source': '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/transcriptions/C5W3L09 SpeechRecog [vm2SI8AJY0s].txt'}, page_content=\"characters and still end up with a much shorter output text transfer. So this phrase here, the quick brown fox, including actually has 19 characters and if somehow the new network is forced up with a thousand characters by allowing the network to insert blanks and repeated characters, and can still represent this 19 character output with this 1,000 output values of Y. So this paper by Alex Gray's as well as by Du's deep speech speech recognition system which I was involved in used this idea to build effective speech recognition systems. So I hope that gives you a rough sense of how speech recognition models work. Attention-like models work and CTC models work and present two different options for how to go about building these systems. Now today building a effective or a production-scale speech recognition system is a pretty significant effort and requires a very large data set. But what I'd like to do in the next video is share of you how you can build a trigger word detection system\"),\n",
       " Document(id='f7ac1eb7-4759-4bbc-a8cd-2a1357ffea9a', metadata={'source': '/Users/wangzeyu/Desktop/Github projects/legalai-chatbot/data/transcriptions/C5W3L02 Picking the most likely sentence [Er2ucMxjdHE].txt'}, page_content=\"find the value of Y that maximizes this, it usually does a good enough job. So to summarize, in this video you saw how machine translation can be posed as a conditional language modeling problem. But one major difference between this and the earlier language modeling problems is rather wanting to generate a sentence at random, you maybe want to try to find the most likely English sentence, the most likely English translation. But the set of all English sentences of a certain length is too large to exhaustively a neural rate, so we'll have to resort to a search algorithm. So with that, let's go on to the next video where you learn about the beam search algorithm.\")]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_res = db.similarity_search(\"What is sequence to sequence model?\", k=10)\n",
    "vector_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Keep the answer as concise as possible.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model='', base_url='', api_key='n')\n",
    "rag_res = model.invoke(\n",
    "    prompt_template.format(\n",
    "        context=''.join([i.page_content for i in vector_res]), \n",
    "        question=\"What is sequence to sequence model?\"\n",
    "    )\n",
    ")\n",
    "print(rag_res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer to https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/ \n",
    "# for building rag chain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
